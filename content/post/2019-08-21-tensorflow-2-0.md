---
title: TensorFlow 2.0
author: ~
date: '2019-08-21'
slug: tensorflow-2-0
categories: ['Data Science']
tags: ['datascience']
comments: yes
image: ''
menu: ''
share: yes
---

This is a blog version of the Knowledge Sharing talk on 2018-08-28 given by Wei-Chun Chu to the Fit Prediction team, Fit Analytics GmbH.

### Main changes from TF-1 to TF-2

- API reorganization
- Eager execution *
- tf.function and AutoGraph *
- Flow control (*)
- Globals
- keras vs. estimator *
- [_Support TensorBoard in Jupyter notebook_](https://github.com/tensorflow/tensorboard/tree/master/docs/r2)

The starred items will be in more details in the following.

### Change codes to TF-2 style

The following writing styles are strongly suggested, if not enforced:

- session runs -> python functions
- named variables -> python objects
- use high-level API (keras etc) for training loops
- tf.data for input
- tf.compat -> new API

[_More tips of writing native TF-2 codes_](https://www.tensorflow.org/beta/guide/migration_guide#make_the_code_20-native)

### Eager execution basics

TF-2 [_executes eagerly_](https://www.tensorflow.org/guide/eager) by default. Why eager execution?

- intuitive
- easy to debug
- natural control flow

Example: build and train a 1D linear regression model (`y = W * x + b`)

Here are the links to the full codes of the [_TF-1 custom model_](tf1-custom.pdf), [_TF-2 custom model_](tf2-custom.pdf), and [_TF-2 keras model_](tf2-keras.pdf).

The following is the core part of the TF-1 model. The roles of placeholders and variables are clearly separated. Any variable and object derived from variables entering session will be part of the graph. Executions are done by `run` in the session, with given feed information if necessary for the execution.

```
# define model
X = tf.placeholder("float")
Y = tf.placeholder("float")
W = tf.Variable(5.0, name="weight")
b = tf.Variable(0.0, name="bias")
pred = tf.add(tf.multiply(X, W), b)

# train model
N_EPOCHS = 100
LEARNING_RATE = 0.1
loss = tf.reduce_mean(tf.square(pred - Y))
optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)
with tf.Session() as sess:
    sess.run(init)
    epochs = range(N_EPOCHS)
    for epoch in epochs:
        sess.run(optimizer, feed_dict={X: inputs, Y: outputs})
```

The following is the core part of the TF-2 model. The `Model` class basically defines a function that returns `W * x + b` with initial values of `W` and `b`. It's written in a pythonic way. The `loss` and `train` functions are also written like normal python functions. In this example, no graph is created. We'll talk about `tf.function` and AutoGraph later.

```
# define model
class Model(object):
    def __init__(self):
        self.W = tf.Variable(5.0)
        self.b = tf.Variable(0.0)

    def __call__(self, x):
        return self.W * x + self.b

model = Model()

# training
N_EPOCHS = 100
LEARNING_RATE = 0.1

def loss(predicted_y, target_y):
    return tf.reduce_mean(tf.square(predicted_y - target_y))

def train(model, inputs, outputs, learning_rate):
    # gradient descent
    with tf.GradientTape() as t:
        current_loss = loss(model(inputs), outputs)
    dW, db = t.gradient(current_loss, [model.W, model.b])
    model.W.assign_sub(learning_rate * dW)
    model.b.assign_sub(learning_rate * db)

epochs = range(N_EPOCHS)
train(model, inputs, outputs, learning_rate=LEARNING_RATE, epochs=N_EPOCHS)
```

The following is the core part of using keras model in TF-2. Keras very easily uses layers to construct a model. In this case we use the `Sequential` class. If the model has to be very flexible, one can subclass `tf.keras.Model` and cusomize it, which will be shown in this blog later.

```
# define model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(
        units=1,
        input_shape=(1, ),
        use_bias=True,
        kernel_initializer=tf.initializers.constant(5.0), # W = 5.0
        bias_initializer=tf.initializers.constant(0.0)) # b = 0.0
])

# train model
N_EPOCHS = 100
LEARNING_RATE = 0.1

model.compile(
    optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=[tf.keras.metrics.MeanSquaredError()])

h = model.fit(inputs,
              outputs,
              verbose=0,
              batch_size=n,
              epochs=N_EPOCHS)
```

### tf.function

[_Guilde link_](https://www.tensorflow.org/beta/guide/autograph);
[_Tutorial link_](https://www.tensorflow.org/beta/tutorials/eager/tf_function);
[_API link_](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function)

The purpose of tf.function is to "creates a callable TensorFlow graph from a Python function", stated in the guide. There are several things to note which I'll explain more in the example code:

- numerical vs. tensor inputs
- autograph
- input signatures
- tracing/staging

TODO: more content

[_Example code_](tf-function.pdf)

### Flow control (?)

TODO: content

### Keras vs. Estimator

I'll use the PairShift project to demonstrate how to use estimator in TF-1 vs. keras in TF-2, both with some customization in building the regressor model. (In the Knowledge Sharing talk, I'll instead take the offset-builder code for example.)

Here's the [_full code_](https://github.com/wcchu/NN-expt/blob/ebac8b5864bd6be8a0366e856f3a47c4fb98a43d/PairShift/pairshift.ipynb) in TF-1 with `tf.estimator` in Jupyter notebook. The core part of the code is the following:

```
# build input function
input_fn_train=tf.estimator.inputs.pandas_input_fn(
        x=x,
        y=y,
        batch_size=100,
        num_epochs=1,
        shuffle=True
    )

# build model function
def model_fn(features, labels, mode, params):
    # split features into ref and tar features
    col1 = params['feature_columns'][0]
    col2 = params['feature_columns'][1]
    feats = {
        'col1':{'item1': features['item1']},
        'col2':{'item2': features['item2']}
    }
    input_layer = {
        'col1': tf.feature_column.input_layer(feats['col1'], col1),
        'col2': tf.feature_column.input_layer(feats['col2'], col2),
    }
    subtracted = tf.subtract(input_layer['col1'], input_layer['col2'])
    out = tf.layers.dense(
        subtracted, units=1,
        use_bias=False,
        kernel_initializer=None)

    # define head
    my_head = tf.contrib.estimator.regression_head(
        label_dimension=1,
        loss_fn=None  # custom loss, default: mean_squared_error
    )

    return my_head.create_estimator_spec(
        features=features,
        mode=mode,
        labels=labels,
        optimizer=tf.train.FtrlOptimizer(params['step_size']),
        logits=out,
    )

# custom estimator
regressor = tf.estimator.Estimator(
    model_fn=model_fn,
    params={'feature_columns': [item1_col, item2_col],
            'step_size': 0.2}
)

# train estimator
regressor.train(
    input_fn=input_fn_train,
    steps=None
)
```

Here is the [_full code_](https://github.com/wcchu/NN-expt/blob/d749c464c65764f291efd9d94f1e515ab367a03d/PairShift/pairshift.py) in TF-2 using keras. The core part of code:

```
# subclass keras model
class PairModel(tf.keras.Model):
    '''
    Train variables using the paired examples
    Tutorial on basic structure:
    https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model
    '''

    def __init__(self, columns):
        super(PairModel, self).__init__()
        self.input1 = tf.keras.layers.DenseFeatures(columns['item1'])
        self.input2 = tf.keras.layers.DenseFeatures(columns['item2'])
        self.subtracted = tf.keras.layers.Subtract(name='sub')
        self.dense = tf.keras.layers.Dense(1, use_bias=False)

    def call(self, inputs):
        # input1 and input2 take "item1" and "item2" respectively from
        # inputs
        hot1 = self.input1(inputs)
        hot2 = self.input2(inputs)
        # subtract the onehot of item2 from onehot of item1
        sub = self.subtracted([hot1, hot2])
        # simple linear regression
        out = self.dense(sub)
        return out

# columns
columns = {
    'item1': tf.feature_column.indicator_column(
        tf.feature_column.categorical_column_with_vocabulary_list(
            'item1', vocabulary_list=items)),
    'item2': tf.feature_column.indicator_column(
        tf.feature_column.categorical_column_with_vocabulary_list(
            'item2', vocabulary_list=items))
}

# create model
pair_model = create_pair_model(columns)

# train model
pair_model.fit(ds_train, validation_data=ds_eval, epochs=EPOCHS)

# print summary
pair_model.summary()
```

TODO: talk about the example
