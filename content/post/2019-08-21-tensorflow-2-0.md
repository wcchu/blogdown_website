---
title: TensorFlow 2.0
author: ~
date: '2019-08-21'
slug: tensorflow-2-0
categories: ['Data Science']
tags: ['datascience']
comments: yes
image: ''
menu: ''
share: yes
---

## Main changes from TF-1 to TF-2

- API reorganization
- Eager execution *
- tf.function and AutoGraph *
- Flow control (*)
- Globals
- keras vs. estimator *
- [_Support TensorBoard in Jupyter notebook_](https://github.com/tensorflow/tensorboard/tree/master/docs/r2)

## Change codes to TF-2 style

[_Tips of writing native TF-2 codes_](https://www.tensorflow.org/beta/guide/migration_guide#make_the_code_20-native)

- session runs -> python functions
- named variables -> python objects
- use high-level API (keras etc) for training loops
- tf.data for input
- tf.compat -> new API

## Eager execution basics

TF-2 [_executes eagerly_](https://www.tensorflow.org/guide/eager) by default.

- An intuitive interface
- Easier debugging
- Natural control flow

Example: build and train a 1D linear regression model (`y = W * x + b`)

[_TF-1 custom model_](tf1-custom.pdf)

Excerpt from code:

```
# define model
X = tf.placeholder("float")
Y = tf.placeholder("float")
W = tf.Variable(5.0, name="weight")
b = tf.Variable(0.0, name="bias")
pred = tf.add(tf.multiply(X, W), b)

# train model
N_EPOCHS = 100
LEARNING_RATE = 0.1
loss = tf.reduce_mean(tf.square(pred - Y))
optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)
with tf.Session() as sess:
    sess.run(init)
    epochs = range(N_EPOCHS)
    for epoch in epochs:
        for (x, y) in zip(inputs, outputs):
            sess.run(optimizer, feed_dict={X: x, Y: y})
```

[_TF-2 custom model_](tf2-custom.pdf)

Excerpt from code:

```
# define model
class Model(object):
    def __init__(self):
        self.W = tf.Variable(5.0)
        self.b = tf.Variable(0.0)

    def __call__(self, x):
        return self.W * x + self.b

model = Model()

# training
N_EPOCHS = 100
LEARNING_RATE = 0.1

def loss(predicted_y, target_y):
    return tf.reduce_mean(tf.square(predicted_y - target_y))

def train(model, inputs, outputs, learning_rate):
    # gradient descent
    with tf.GradientTape() as t:
        current_loss = loss(model(inputs), outputs)
    dW, db = t.gradient(current_loss, [model.W, model.b])
    model.W.assign_sub(learning_rate * dW)
    model.b.assign_sub(learning_rate * db)

epochs = range(N_EPOCHS)
train(model, inputs, outputs, learning_rate=LEARNING_RATE, epochs=N_EPOCHS)
```

[_TF-2 keras model_](tf2-keras.pdf)

Excerpt from the code:

```
# define model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(
        units=1,
        input_shape=(1, ),
        use_bias=True,
        kernel_initializer=tf.initializers.constant(5.0), # W = 5.0
        bias_initializer=tf.initializers.constant(0.0)) # b = 0.0
])

# train model
N_EPOCHS = 100
LEARNING_RATE = 0.1

model.compile(
    optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=[tf.keras.metrics.MeanSquaredError()])

h = model.fit(inputs,
              outputs,
              verbose=0,
              batch_size=n,
              epochs=N_EPOCHS)
```

## tf.function

[_Guilde link_](https://www.tensorflow.org/beta/guide/autograph);
[_Tutorial link_](https://www.tensorflow.org/beta/tutorials/eager/tf_function);
[_API link_](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function)

"Creates a callable TensorFlow graph from a Python function."

- numerical vs. tensor inputs
- autograph
- input signatures
- tracing/staging

[_Example code_](tf-function.pdf)

## Flow control (?)

## Keras vs. Estimator

I'll use the offset training model to demonstrate how to use estimator in TF-1 vs. keras in TF-2, both with some customization in building the regressor model.
