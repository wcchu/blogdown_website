---
title: TensorFlow 2.0
author: ~
date: '2019-08-23'
slug: tensorflow-2-0
categories: ['Data Science']
tags: ['datascience']
comments: yes
image: ''
menu: ''
share: yes
---

This is a blog version of the Knowledge Sharing talk to be given by Wei-Chun Chu on 2018-08-28 to the Fit Prediction team, Fit Analytics GmbH. The purpose is to sum up the important changes from TensorFlow 1.* (TF-1) to 2.0 (TF-2), and to give useful tips in writing codes in TF-2.

### Main changes from TF-1 to TF-2

- API reorganization
- Eager execution *
- tf.function and AutoGraph *
- Flow control
- Globals
- Keras vs. Estimator *
- [Support TensorBoard in Jupyter notebook](https://github.com/tensorflow/tensorboard/tree/master/docs/r2)

The starred items will be in more details in the following.

### Change codes to TF-2 style

The following writing styles are strongly suggested, if not enforced:

- session runs -> python functions
- named variables -> python objects
- use high-level API (Keras etc) for training loops
- tf.data for input
- tf.compat -> new API

[More tips of writing native TF-2 codes](https://www.tensorflow.org/beta/guide/migration_guide#make_the_code_20-native)

### Eager execution basics

TF-2 [executes eagerly](https://www.tensorflow.org/guide/eager) by default. Why eager execution?

- intuitive
- easy to debug
- natural control flow

Example: build and train a 1D linear regression model (`y = W * x + b`)

Here are the links to the full codes of the [TF-1 custom model](tf1-custom.pdf), [TF-2 custom model](tf2-custom.pdf), and [TF-2 Keras model](tf2-keras.pdf).

The following is the core part of the TF-1 model. The roles of placeholders and variables are clearly separated. Any variable and object derived from variables entering session will be part of the graph. Executions are done by `run` in the session, with given feed information if necessary for the execution.

```
# define model
X = tf.placeholder("float")
Y = tf.placeholder("float")
W = tf.Variable(5.0, name="weight")
b = tf.Variable(0.0, name="bias")
pred = tf.add(tf.multiply(X, W), b)

# train model
N_EPOCHS = 100
LEARNING_RATE = 0.1
loss = tf.reduce_mean(tf.square(pred - Y))
optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)
with tf.Session() as sess:
    sess.run(init)
    epochs = range(N_EPOCHS)
    for epoch in epochs:
        sess.run(optimizer, feed_dict={X: inputs, Y: outputs})
```

The following is the core part of the TF-2 model. The `Model` class basically defines a function that returns `W * x + b` with initial values of `W` and `b`. It's written in a pythonic way. The `loss` and `train` functions are also written like normal python functions. In this example, no graph is created. We'll talk about `tf.function` and AutoGraph later.

```
# define model
class Model(object):
    def __init__(self):
        self.W = tf.Variable(5.0)
        self.b = tf.Variable(0.0)

    def __call__(self, x):
        return self.W * x + self.b

model = Model()

# training
N_EPOCHS = 100
LEARNING_RATE = 0.1

def loss(predicted_y, target_y):
    return tf.reduce_mean(tf.square(predicted_y - target_y))

def train(model, inputs, outputs, learning_rate):
    # gradient descent
    with tf.GradientTape() as t:
        current_loss = loss(model(inputs), outputs)
    dW, db = t.gradient(current_loss, [model.W, model.b])
    model.W.assign_sub(learning_rate * dW)
    model.b.assign_sub(learning_rate * db)

epochs = range(N_EPOCHS)
train(model, inputs, outputs, learning_rate=LEARNING_RATE, epochs=N_EPOCHS)
```

The following is the core part of using Keras model in TF-2. Keras very easily uses layers to construct a model. In this case we use the `Sequential` class. If the model has to be very flexible, one can subclass `tf.keras.Model` and cusomize it, which will be shown in this blog later.

```
# define model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(
        units=1,
        input_shape=(1, ),
        use_bias=True,
        kernel_initializer=tf.initializers.constant(5.0), # W = 5.0
        bias_initializer=tf.initializers.constant(0.0)) # b = 0.0
])

# train model
N_EPOCHS = 100
LEARNING_RATE = 0.1

model.compile(
    optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=[tf.keras.metrics.MeanSquaredError()])

h = model.fit(inputs,
              outputs,
              verbose=0,
              batch_size=n,
              epochs=N_EPOCHS)
```

### tf.function

[Guilde link](https://www.tensorflow.org/beta/guide/autograph);
[Tutorial link](https://www.tensorflow.org/beta/tutorials/eager/tf_function);
[API link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function)

The purpose of tf.function is to "creates a callable TensorFlow graph from a Python function", stated in the guide. There are several things to note which I'll explain more in the example code:

- numerical vs. tensor inputs
- autograph
- input signatures
- tracing/staging

[Example code](tf-function.pdf)

### Keras vs. Estimator

In TF-1, both Estimator and Keras are high-level APIs used to build, train, and analyze common models. In TF-2, Keras is the suggested high-level API. It has a lot of convenient functionalities and flexibility for customization. I'll use the [PairShift](https://github.com/wcchu/NN-expt/tree/master/PairShift) project to demonstrate how to use Estimator in TF-1 vs. Keras in TF-2, both with some customization in building the regressor model. (In the Knowledge Sharing talk, I'll instead take the offset-builder code for example.) In each case, I'll show the code first followed by some discussion.

#### Estimator in TF-1

Here's the [full code](https://github.com/wcchu/NN-expt/blob/ebac8b5864bd6be8a0366e856f3a47c4fb98a43d/PairShift/pairshift.ipynb) in TF-1 with `tf.estimator` in Jupyter notebook. The core part of the code is the following:

```
# columns
item1_col = tf.feature_column.indicator_column(
    tf.feature_column.categorical_column_with_vocabulary_list(
        'item1', vocabulary_list=list_items))
item2_col = tf.feature_column.indicator_column(
    tf.feature_column.categorical_column_with_vocabulary_list(
        'item2', vocabulary_list=list_items))
dif_col = tf.feature_column.numeric_column('dif')

# build input function
input_fn_train=tf.estimator.inputs.pandas_input_fn(
        x=x,
        y=y,
        batch_size=100,
        num_epochs=1,
        shuffle=True
    )

# build model function
def model_fn(features, labels, mode, params):
    # split features into ref and tar features
    col1 = params['feature_columns'][0]
    col2 = params['feature_columns'][1]
    feats = {
        'col1':{'item1': features['item1']},
        'col2':{'item2': features['item2']}
    }
    input_layer = {
        'col1': tf.feature_column.input_layer(feats['col1'], col1),
        'col2': tf.feature_column.input_layer(feats['col2'], col2),
    }
    subtracted = tf.subtract(input_layer['col1'], input_layer['col2'])
    out = tf.layers.dense(
        subtracted, units=1,
        use_bias=False,
        kernel_initializer=None)

    # define head
    my_head = tf.contrib.estimator.regression_head(
        label_dimension=1,
        loss_fn=None  # custom loss, default: mean_squared_error
    )

    return my_head.create_estimator_spec(
        features=features,
        mode=mode,
        labels=labels,
        optimizer=tf.train.FtrlOptimizer(params['step_size']),
        logits=out,
    )

# custom estimator
regressor = tf.estimator.Estimator(
    model_fn=model_fn,
    params={'feature_columns': [item1_col, item2_col],
            'step_size': 0.2}
)

# train estimator
regressor.train(
    input_fn=input_fn_train,
    steps=None
)
```

The model class in Estimator is `tf.estimator.Estimator`, which takes in a model function and additional parameters. The operations in the model are defined in the model function. Note that the seemingly complicated and confusing column- and feature-related objects in the first few lines of `model_fn()` are in compliance with the format of [`tf.feature_column.input_layer`](https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer). Once the input layer(s) are defined, the following operations are written as mathematical operations, as shown by using `tf.subtract` to create the `subtracted` layer. The use of `tf.contrib.estimator.regression_head` shows that Estimator's model customization is unfortunately not very mature.

#### Keras in TF-2

Now we move on to building the same model with Keras in TF-2 style. Here is the [full code](https://github.com/wcchu/NN-expt/blob/d749c464c65764f291efd9d94f1e515ab367a03d/PairShift/pairshift.py). The core part of code:

```
# subclass Keras model
class PairModel(tf.keras.Model):
    '''
    Train variables using the paired examples
    Tutorial on basic structure:
    https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model
    '''

    def __init__(self, columns):
        super(PairModel, self).__init__()
        self.input1 = tf.keras.layers.DenseFeatures(columns['item1'])
        self.input2 = tf.keras.layers.DenseFeatures(columns['item2'])
        self.subtracted = tf.keras.layers.Subtract(name='sub')
        self.dense = tf.keras.layers.Dense(1, use_bias=False)

    def call(self, inputs):
        # input1 and input2 take "item1" and "item2" respectively from
        # inputs
        hot1 = self.input1(inputs)
        hot2 = self.input2(inputs)
        # subtract the onehot of item2 from onehot of item1
        sub = self.subtracted([hot1, hot2])
        # simple linear regression
        out = self.dense(sub)
        return out

# columns
columns = {
    'item1': tf.feature_column.indicator_column(
        tf.feature_column.categorical_column_with_vocabulary_list(
            'item1', vocabulary_list=items)),
    'item2': tf.feature_column.indicator_column(
        tf.feature_column.categorical_column_with_vocabulary_list(
            'item2', vocabulary_list=items))
}

def create_pair_model(columns):
    '''
    Define pair model
    '''
    model = PairModel(columns)
    model.compile(
        optimizer=tf.keras.optimizers.RMSprop(0.001),
        loss='mse',
        metrics=['mae', 'mse'],
        run_eagerly=False)
    return model

# create model
pair_model = create_pair_model(columns)

# train model
pair_model.fit(ds_train, validation_data=ds_eval, epochs=EPOCHS)

# print summary
pair_model.summary()
```

The `tf.keras.Model` class has the bare minimal of a Keras model and we subclass for our own model class. The layers and operations are defined inside the `PairModel` class, and the subtraction operation in Estimator in the first example is replaced by `tf.keras.layers.Subtract` here. Keras provides many layer operations in the [`tf.keras.layers` API](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers) and I recommend reading through it before turning to low-level model if any customization is needed. Another thing to notice is how clean it is to specify the input features in the model; the columns assigned by `tf.feature_columns` are simply passed into the layers in the `__init__` part of `PairModel`. It's much easier to read and debug. In TF-2, Keras can be run with eager or graph modes by specifying `run_eagerly` when compiling the model. A possible issue is, in eager mode, the training will build graphs on the fly, and the graphs will pile up during the training process, possibly consuming a lot of memory and clogging the pipeline (see discussion [here](https://github.com/tensorflow/tensorflow/issues/30561#issuecomment-510399901)). It is thus suggested to turn off the eager mode after debugging.
